{
  "name": "wscraper",
  "description": "wscraper.js: a web scraper agent based on cheerio.js a fast, flexible, and lean implementation of core jQuery; built on top of request.js; inspired by http-agent.js;",
  "version": "0.1.0",
  "maintainers": [
    {
      "name": "kalise",
      "email": "adkalise@gmail.com"
    }
  ],
  "author": {
    "name": "kalise"
  },
  "main": "./lib/wscraper",
  "dependencies": {
    "request": "~2.10.0",
    "cheerio": "~0.9.2",
    "iconv": "~1.2.3"
  },
  "devDependencies": {},
  "readme": "# wscraper\n\nwscraper.js is a web scraper agent written in node.js and based on [cheerio.js][0] a fast, flexible, and lean implementation of core jQuery;\nIt is built on top of [request.js][1] and inspired by [http-agent.js][2];\n\n## Usage \n\nThere are two ways to use wscraper: http agent mode and local mode. \n\n### HTTP Agent mode\nIn HTTP Agent mode, pass it a host, a list of URLs to visit and a scraping JS script. For each URLs, the agent makes a request, gets the response, runs the scraping script and returns the result of the scraping. Valid usage is:\n\n```js\n// scrape a single page from a web site\nvar agent = wscraper.createAgent();\nagent.start('google.com', '/finance', script);\n\n// scrape multiple pages from a website\nwscraper.start('google.com', ['/', '/finance', '/news'], script);\n```\n\nThe URLs should be passed as an array of strings. In case only one page needs to be scraped, the URL can be passed as a single string. Null or empty URLs are treated as  root '/'. Suppose you want to scrape from http://google.com/finance website the stocks price of the following companies: Apple, Cisco and Microsoft.\n\n```js\n// load node.js libraries\nvar\tutil = require('util');\nvar\twscraper = require('wscraper');\nvar\tfs = require('fs');\n\n// load the scraping script from a file\nvar script = fs.readFileSync('/scripts/googlefinance.js');\n\nvar companies = ['/finance?q=apple', '/finance?q=cisco', '/finance?q=microsoft'];\n\n// create a web scraper agent instance\nvar agent = wscraper.createAgent();\n\nagent.on('start', function (n) {\n\tutil.log('[wscraper.js] agent has started; ' + n + ' path(s) to visit');\n});\n\nagent.on('done', function (url, price) {\n\tutil.log('[wscraper.js] data from ' + url);\n\t// display the results\t\n\tutil.log('[wscraper.js] current stock price is ' + price + ' USD');\n\t// next item to process if any\n\tagent.next();\t\t\n});\n\nagent.on('stop', function (n) {\n\tutil.log('[wscraper.js] agent has ended; ' + n + ' path(s) remained to visit');\n});\n\nagent.on('abort', function (e) {\n\tutil.log('[wscraper.js] getting a FATAL ERROR [' + e + ']');\n\tutil.log('[wscraper.js] agent has aborted');\n\tprocess.exit();\n});\n\n// run the web scraper agent\nagent.start('www.google.com', companies, script);\n```\n\nThe scraping script should be pure client JavaScript, including JQuery selectors. See [cheerio.js][0] for details. I should return a valid JavaScript object.\nThe scraping script is passed as a string and usually is read from a file. You can scrape different websites without change any line of the main code: only write different JavaScript scripts.\nThe scraping script is executed in a sandbox using a separate VM context and the script errors are caught without crash of the main code.\n\nAt time of writing, google.com/finance website reports financial data of public companies as in the following html snippet:\n\n```html\n...\n<div id=\"price-panel\" class=\"id-price-panel goog-inline-block\">\n  <div>\n    <span class=\"pr\">\n  \t<span id=\"ref_22144_l\">656.06</span>\n    </span>\n  </div>\n</div>\n...\n```\nBy using JQuery selectors, we design the scraping script \"googlefinance.js\" to find the current value of a company stocks and return it as a text:\n\n```js\n/*\n\ngooglefinance.js\n\n$ -> is the DOM document to be parsed\nresult -> is the object containing the result of parsing\n*/\n\nresult = {};\nprice = $('div.id-price-panel').find('span.pr').children().text();\nresult.price = price;\n\n// result is '656.06'\n```\n\n### Local mode\nSometimes, you need to scrape local html files without make a request to a remote server. Wscraper can be used as inline scraper. It takes an html string and a JS scraping script. The scraper runs the scraping script and returns the result of the scraping. Valid usage is:\n\n```js\nvar scraper = wscraper.createScraper();\nscraper.run(html, script);\n```\n\nOnly as trivial example, suppose you want to replace the class name of <div> elements only containing an image with a given class. Create a scraper:\n\n```js\n// load node.js libraries\nvar\tutil = require('util');\nvar\tfs = require('fs');\nvar\twscraper = require('wscraper');\n\n// load your html page\nvar html = fs.readFileSync('/index.html');\n\n// load the scraping script from a file\nvar script = fs.readFileSync('/scripts/replace.js');\n\n// create the scraper\nvar scraper = wscraper.createScraper();\n\nscraper.on('done', function(result) {\n\t// do something with the result\n\tutil.log(result)\n});\n\nscraper.on('abort', function(e) {\n\tutil.log('Getting error in parsing: ' + e)\n});\n\n// run the scraper\nscraper.run(html, script);\n```\n\nBy using JQuery selectors, we design the scraping script \"replace.js\" to find the <div> elements containing images with class=\"MyPhotos\" and replace each of them with a <div> element having class=\"Hidden\" without any image inside.\n\n```js\n/*\nreplace.js\n\n$ -> is the DOM document to be parsed\nresult -> is the final JSON string containing the result of parsing\nuse var js-obj = JSON.parse(result) to get a js object from the json string\nuse JSON.stringify(js-obj) to get back a json string from the js object\n*/\n\nresult = {};\nvar imgs = $('img.MyPhotos').toArray();\n$.each(imgs, function(index, elem) {\n\tvar parentdiv = $(elem).parent();\n\tvar newdiv = $('<div class=\"Hidden\"/></div>');\n\t$(elem).parent().replaceWith(newdiv)\n});\n\nresult.replaced = $.html() || '';\n```\n\nHappy scraping!\n\n### Author: kalise Â© 2012 MIT Licensed;\n\n[0]: https://github.com/MatthewMueller/cheerio\n[1]: https://github.com/mikeal/request\n[2]: https://github.com/indexzero/http-agent\n",
  "_id": "wscraper@0.1.0",
  "dist": {
    "shasum": "f359bdec9e20f91a0b7fe42df79b89d9c049a652",
    "tarball": "http://registry.npmjs.org/wscraper/-/wscraper-0.1.0.tgz"
  },
  "directories": {},
  "_shasum": "f359bdec9e20f91a0b7fe42df79b89d9c049a652",
  "_resolved": "https://registry.npmjs.org/wscraper/-/wscraper-0.1.0.tgz",
  "_from": "wscraper@*"
}
